library("EBImage")
setwd("./ads_spr2017_proj3")
experiment_dir <- "../data/zipcode/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep=""),
header=F)
label_train <- as.numeric(unlist(label_train) == "9")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zipcode",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir,
"test",
data_name="zip",
export=TRUE))
#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir,
"test",
data_name="zip",
export=TRUE))
}
#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(depth_values), 2))
for(k in 1:length(depth_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(depth_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
experiment_dir <- "../data/zipcode/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep=""),
header=F)
label_train <- as.numeric(unlist(label_train) == "9")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir,
"test",
data_name="zip",
export=TRUE))
}
#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.15))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2],depth_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.15))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.25))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
model_best=model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(par=model_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
View(err_cv)
which.min(err_cv[,1])
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
fit_train <- train(dat_train, label_train, par_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par=par_best))
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par=par_best))
par_best$par
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
source('~/Dropbox/Tian_Teaching/G5243-ADS/0-Projects-startercodes/3-Spring2017/Project3_PoodleKFC/lib/train.R')
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
source('~/Dropbox/Tian_Teaching/G5243-ADS/0-Projects-startercodes/3-Spring2017/Project3_PoodleKFC/lib/train.R')
model_best=model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(depth=model_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
save(fit_train, file="../output/fit_train.RData")
tm_test=NA
if(run.test){
load(file=paste0("../output/feature_", "zip", "_", "test", ".RData"))
load(file="../output/fit_train.RData")
tm_test <- system.time(pred_test <- test(fit_train, dat_test))
save(pred_test, file="../output/pred_test.RData")
}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
require("pacman")
install.packages("pacman")
if (!require("pacman")) {
## Make sure your current packages are up to date
update.packages()
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
if (!require("pacman")) {
## Make sure your current packages are up to date
update.packages()
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages(c("acs", "backports", "bayesplot", "BH", "bit", "bit64", "blogdown", "bookdown", "boot", "broom", "cairoDevice", "car", "caTools", "checkmate", "choroplethr", "chron", "cli", "cluster", "coin", "colourpicker", "countrycode", "curl", "data.table", "dendextend", "devtools", "digest", "doParallel", "dplyr", "DT", "dtplyr", "dygraphs", "ellipse", "evaluate", "factoextra", "FactoMineR", "fansi", "fftwtools", "flexmix", "foreach", "foreign", "formatR", "Formula", "fpc", "gbm", "gdata", "gender", "geosphere", "ggplot2", "ggpubr", "ggrepel", "ggsci", "git2r", "gridExtra", "gtools", "highr", "Hmisc", "htmlTable", "htmlwidgets", "httpuv", "hunspell", "igraph", "inline", "irlba", "iterators", "janeaustenr", "kernlab", "knitr", "lattice", "lazyeval", "lme4", "lmtest", "loo", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "matrixStats", "mclust", "memoise", "mgcv", "mime", "miniUI", "modeltools", "multcomp", "munsell", "mvtnorm", "nloptr", "NLP", "NMF", "OAIHarvester", "openNLPdata", "openssl", "packrat", "pbkrtest", "pkgmaker", "PKI", "plotrix", "ps", "psych", "qdap", "qdapDictionaries", "qdapRegex", "qdapTools", "quantreg", "R6", "randomForest", "RANN", "raster", "Rcpp", "RcppEigen", "RCurl", "registry", "remotes", "reprex", "reshape2", "rgdal", "rgeos", "RgoogleMaps", "rJava", "rjson", "rlang", "rmarkdown", "rngtools", "robustbase", "rpart", "rpart.plot", "rprojroot", "rsconnect", "rscopus", "rstantools", "rstudioapi", "sandwich", "scales", "scatterplot3d", "selectr", "servr", "shiny", "shinyjs", "shinystan", "sm", "sourcetools", "sp", "SparseM", "statmod", "stringdist", "stringi", "stringr", "survival", "syuzhet", "testthat", "TH.data", "threejs", "tidyr", "tidyselect", "tidytext", "tm", "tokenizers", "topicmodels", "trimcluster", "viridis", "WDI", "withr", "wordcloud", "xlsx", "XML", "xml2", "xtable", "xts", "yaml", "zoo"))
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}y
install.packages("pacman")
library(packman)
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages("devtools")
install.packages("devtools")
install.packages("yaml")
install.packages(c("bit", "cairoDevice", "caTools", "chron", "curl", "data.table", "digest", "dplyr", "fansi", "foreign", "gbm", "ggplot2", "ggrepel", "git2r", "gtools", "igraph", "irlba", "kernlab", "lattice", "lme4", "lmtest", "mapproj", "maps", "maptools", "MASS", "Matrix", "matrixStats", "mclust", "mgcv", "mime", "mvtnorm", "nloptr", "NMF", "openssl", "ps", "quantreg", "randomForest", "RANN", "raster", "Rcpp", "RcppEigen", "RCurl", "rgdal", "rgeos", "rJava", "rjson", "rlang", "robustbase", "rstan", "rstanarm", "scales", "sentimentr", "slam", "sm", "sourcetools", "sp", "StanHeaders", "stringdist", "stringi", "stringr", "survival", "testthat", "tidyr", "tidyselect", "tm", "tokenizers", "wordcloud", "XML", "xml2", "xts", "zoo"))
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages("devtools")
library(devtools)
install.packages("base64enc")
install.packages("devtools")
install.packages("processx")
install.packages("devtools")
install.packages("backports")
install.packages("devtools")
install.packages("glue")
install.packages("devtools")
library(devtools)
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #97 files in total
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt","",file_name_vec[3])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
file_name_vec[5]\
file_name_vec[5]
?paste
paste("1st", "2nd", "3rd", collapse = ", ")
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt","",file_name_vec[5])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] #1124 tokens
tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken)) # source code of ifCleanToken in in lib folder
tesseract_if_clean
tesseract_vec
?str_count
?gsub
a1 <- " aaaaaBlE"
?pmatch
unlist(strsplit(a1,''))
a1 <- "aaaaaBlE"
a2 <- unlist(strsplit(a1,''))
a2
?grep
nchar(a1)>20
grepl(pattern=[a-z\\d]\\1\\1,a1)
grepl(pattern='[a-z\\d]\\1\\1',a1)
length(unique(as.vector(unlist(strsplit(a1,"")),mode="list")))
a1
a2
unique(a2)
length(unique(a2))
grepl(pattern='[a-z]{3}',a1)
a1
a3 <- "abc"
grepl(pattern='[a-z]{3}',a3)
grepl(pattern='[a-z]*([a-z])\1([a-z])\2[a-z]*',a1)
grepl(pattern='[a-z]*([a-z])\1([a-z])\2[a-z]*',a3)
a1
grepl(pattern='[a-z]*([a-z])\1([a-z])\2[a-z]*',a1, perl = TRUE)
a1
grepl(pattern='[a-z]\3+',a1)
grepl(pattern='(.)\3+',a1)
grepl(pattern='(.)\2+',a1)
a1
grepl(pattern='[a-z]\2+',a1)
grepl(pattern='\w\2+',a1)
grepl(pattern='(\w)\2+',a1)
grepl(pattern='([a-z]\\2+)',a1)
grepl(pattern='([a-z])\\2+',a1)
grepl(pattern='(.)\1{2,}',a1)
a1
grepl(pattern='(.)\\1{2,}',a1)
a2 <- 'aaBlE'
grepl(pattern='(.)\\1{2,}',a2)
a3 <- 'aaaBlE'
grepl(pattern='(.)\\1{2,}',a3)
b1 <- " BBEYaYYq"
str_count(b1,pattern='[A-Z]')>str_count(b1,pattern='[a-z]')
str_count(b1,pattern='[A-Z]')>str_count(b1,pattern='[a-z]') AND str_count(b1,pattern='[A-Z]')<str_count(b1,pattern='[A-Za-z0-9]')
str_count(b1,pattern='[A-Z]')<str_count(b1,pattern='[A-Za-z0-9]')
str_count(b1,pattern='[A-Z]')>str_count(b1,pattern='[a-z]') & str_count(b1,pattern='[A-Z]')<str_count(b1,pattern='[A-Za-z0-9]')
c1 <- "jabwqbpP"
grepl("^[A-Za-z]+$",c1)
str_count(b1,pattern='[b-df-hj-np-tv-z]')>8*str_count(b1,pattern='[aeiou]')
b1
str_count(b1,pattern='[b-df-hj-np-tv-z]')
str_count(b1,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]')
c1
str_count(c1,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]')
str_count(c1,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]') >8*str_count(c1,pattern='[aeiouAEIOU]')
str_count(c1,pattern='[aeiouAEIOU]')
str_count(c1)
c2 <- "jjabwqbpP"
grepl("^[A-Za-z]+$",c2) & (str_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]') >8*str_count(c2,pattern='[aeiouAEIOU]') | str_count(c2,pattern='[aeiouAEIOU]') >8*str_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]'))
grepl("^[A-Za-z]+$",c2)
str_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]') >8*str_count(c2,pattern='[aeiouAEIOU]')
str_count(c2)
tr_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]')
str_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]')
grepl("^[A-Za-z]+$",c2) & (str_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]') > = 8*str_count(c2,pattern='[aeiouAEIOU]') | str_count(c2,pattern='[aeiouAEIOU]') > = 8*str_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]'))
str_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]') >= 8*str_count(c2,pattern='[aeiouAEIOU]')
grepl("^[A-Za-z]+$",c2) & (str_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]') >= 8*str_count(c2,pattern='[aeiouAEIOU]') | str_count(c2,pattern='[aeiouAEIOU]') >= 8*str_count(c2,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]'))
grepl(pattern='([aeiouAEIOU])\\1{3,}',"buauub")
grepl(pattern='[aeiouAEIOU]{3,}',"buauub")
d1 <- "eeeiya"
grepl(pattern='[aeiouAEIOU]{3,}',d1)
d1 <- "eEIeya"
grepl(pattern='[aeiouAEIOU]{3,}',d1)
grepl(pattern='[aeiouAEIOU]{3,}',d1) | grepl(pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]{4,}',d1)
?isUppercase
?toupper
grepl("^[[:lower:]]+$", "awwgrapHic")
grepl("^[a-z].*[a-z]$","awwgrapHic")
a2 <- "cbA"
grepl("^[a-z].*[a-z]$",a2)
grepl("^(.).*(\\d).*$",a2,PERL=TRUE)
grepl("^(.).*(\\d).*$",a2,perl = T)
grep("^(.).*(\\d).*$",a2,perl = =TRUE")
grep("^(.).*(\\d).*$",a2,perl =TRUE")
## in order to accelerate the computation, conduct ealy stopping
rule_list <- c("str_count(cur_token, pattern = '[A-Za-z0-9]') <= 0.5*nchar(cur_token)", # If the number of punctuation characters in a string is greater than the number of alphanumeric characters, it is garbage
"length(unique(strsplit(gsub('[A-Za-z0-9]','',substr(cur_token, 2, nchar(cur_token)-1)),'')[[1]]))>1", #Ignoring the first and last characters in a string,
#if there are two or more different punctuation characters in the string, it is garbage
"nchar(cur_token)>20",#A string composed of more than 20 symbols is garbage
"grepl(pattern='(.)\\1{2,}',cur_token)",
#If there are three or more identical characters in a row in a string, it is garbage
"str_count(cur_token,pattern='[A-Z]')>str_count(cur_token,pattern='[a-z]') & str_count(cur_token,pattern='[A-Z]')<str_count(cur_token,pattern='[A-Za-z0-9]')",
#If the number of uppercase characters in a string is greater than the number of lowercase characters, and if the number of uppercase characters is less than the total number of characters in the string, it is garbage
"grepl("^[A-Za-z]+$",cur_token) & (str_count(cur_token,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]') >= 8*str_count(cur_token,pattern='[aeiouAEIOU]') | str_count(cur_token,pattern='[aeiouAEIOU]') >= 8*str_count(cur_token,pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]'))",
# If all the characters in a string are alphabetic, and if the number of consonants in the string is greater than 8 times the number of vowels in the string, or vice-versa, it is garbage
"grepl(pattern='[aeiouAEIOU]{3,}',cur_token) | grepl(pattern='[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]{4,}',cur_token)",
# If there are four or more consecutive vowels in the string or five or more consecutive consonants in the string, it is garbage
"grepl("^[a-z].*[a-z]$",cur_token) & "
while((if_clean == TRUE)&now<=length(rule_list)){
if(eval(parse(text = rule_list[now]))){
if_clean <- FALSE
}
now <- now + 1
}
return(if_clean)
}
grep("^(.).*(\\d).*$",a2,perl = T)
grep("^(.).*$",a2)
?grep
a2
sub("^(.).*$",a2)
repexpr("^(.).*$",a2)
regexpr("^(.).*$", a2)
gsub('[A-Za-z0-9]',"bLack")
gsub('[A-Za-z0-9]','',"bLack")
grep('^.[a-zA-Z]|[a-zA-Z0-9]$',"bLack")
grep('^.([a-zA-Z].*[a-zA-Z]).$',"bLack")
grepl('\\B[A-Z]+\\B',"bLack")
grepl('\\B[A-Z]+\\B','Black')
grepl('\\B[A-Z]+\\B','blacK')
### only process one of the files in the folder as an example, in your project, you need to use all the files
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] #1124 tokens
tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken)) # source code of ifCleanToken in in lib folder
}
tesseract_if_clean
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
### only process one of the files in the folder as an example, in your project, you need to use all the files
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] #1124 tokens
tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken)) # source code of ifCleanToken in in lib folder
}
