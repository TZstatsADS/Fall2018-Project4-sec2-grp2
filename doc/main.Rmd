---
title: 'Optical character recognition (OCR)'
output: html_notebook
---

Jing Wu

GU4243/GR5243: Applied Data Science

<style type="text/css">
h1.title {
  font-size: 24px;
  color: Black;
}
h1 { /* Header 1 */
  font-size: 24px;
  color: Black;
}
h2 { /* Header 2 */
  font-size: 20px;
  color: Black;
}
h3 { /* Header 3 */
  font-size: 16px;
  color: Black;
}
h4 { /* Header 4 */
  font-size: 14px;
  color: Grey;
}
</style>
# Introduction {-}

Optical character recognition (OCR) is the process of converting scanned images of machine printed or
handwritten text (numerals, letters, and symbols), into machine readable character streams, plain (e.g. text files) or formatted (e.g. HTML files). As shown in Figure 1, the data *workflow* in a typical OCR system consists of three major stages:

* Pre-processing

* OCR character recognition

* Post-processing

![](../figs/ocr_flowchart.png) 

We have processed raw scanned images through the first two steps are relying on the [Tessearct OCR machine](https://en.wikipedia.org/wiki/Tesseract_(software)). R package tutorial can be found [here](https://www.r-bloggers.com/the-new-tesseract-package-high-quality-ocr-in-r/). 

BUT this is not the FOCUS of this project!!!

In this project, we are going to **focus on the third stage -- post-processing**, which includes two tasks: *error detection* and *error correction*.  

# Step 0 - set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) Compute new OCR Confusion Probability Matrix and unique letters set for Cfs_matrix
```{r controls}
COMPUTE.CFS=FALSE
```


# Step 1 - Load library and source code
```{r, warning=FALSE, message = FALSE}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
  ## devtools is required
  library(devtools)
  install_github("trinker/pacman")
}

pacman::p_load(knitr, readr, stringr, tesseract, vecsets)

if (!require("stringdist")) install.packages("stringdist");library(stringdist)
if (!require("R.oo")) install.packages("R.oo");library(R.oo)
if (!require("dplyr")) install.packages("dplyr");library(dplyr)
if (!require("tm")) install.packages("tm");library(tm)

source('../lib/ifCleanToken.R')
source('../lib/measurement.R')
source('../lib/measurement_correct.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
```

# Step 2 - read the files and conduct Tesseract OCR

Although we have processed the Tesseract OCR and save the output txt files in the `data` folder, we include this chunk of code in order to make clear the whole pipeline to you.

```{r, eval=FALSE}
for(i in c(1:length(file_name_vec))){
  current_file_name <- sub(".txt","",file_name_vec[i])
  ## png folder is not provided on github (the code is only on demonstration purpose)
  current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
  
  ### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
  clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
  clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
  
  ### save tesseract text file
  writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
```

# Step 3 - Error detection

Now, we are ready to conduct post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or *incorrectly processed words* -- check to see if an input string is a valid dictionary word or if its n-grams are all legal.

The referenced papers are:

1. [Rule-based techniques](http://webpages.ursinus.edu/akontostathis/KulpKontostathisFinal.pdf)

- rules are in the section 2.2 

2. [Letter n-gram](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1672564)

- focus on positional binary digram in section 3-a.error detection

3. Probabilistic techniques -- [SVM garbage detection](https://dl.acm.org/citation.cfm?doid=2034617.2034626)

- features are in section 5 (you can choose not to implement ‘Levenshtein distance’ feature)


In this statercode, we implement the first three rules in the first paper -- rule based techniques, as an example.

```{r, warning=FALSE}
#Error detection
ifcleanlist <- vector('list', 100)
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])


## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE,encoding = "UTF-8")

## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE,encoding = "UTF-8")
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")

## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] 
tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken))
ifcleanlist[[i]] <- tesseract_if_clean

##Remove error
tesseract_delete_error_vec <- tesseract_vec[tesseract_if_clean]
writeLines(tesseract_delete_error_vec, paste("../output/delete_error/",current_file_name,".txt",sep=""),useBytes = T)

##Save errors in a folder
tesseract_error_vec <- tesseract_vec[!tesseract_if_clean]
writeLines(tesseract_error_vec, paste("../output/error/",current_file_name,".txt",sep=""),useBytes = T)
}
save(ifcleanlist, file = '../output/ifcleanlist.RData')
```

# Step 4 - Error correction

Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates.

The referenced papers are:

1. [Letter n-gram](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1672564}{positional binary digram)

- focus on section 3-b.error correction

2. Supervised model -- [correction regressor](https://arxiv.org/pdf/1611.06950.pdf)

3. [probability scoring without context](https://link.springer.com/content/pdf/10.1007%2FBF01889984.pdf)

- focus on section 3

4. [probability scoring with contextual constraints](https://link.springer.com/content/pdf/10.1007%2FBF01889984.pdf)

- focus on section 5

5. [topic models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4377099)

Here, in our code, we just simply remove the detected-errors.


## Compute the OCR Confusion Matrix
```{r ConfusionMatrix}
## read the ocr text
path = "../data/tesseract/"
file_names = dir(path)
ocr_dir = paste(path, file_names, sep = '')
## read the ground truth
path = "../data/ground_truth/"
# file_names = dir(path)
truth_dir = paste(path, file_names, sep = '')

if(COMPUTE.CFS){
  source("../lib/ConfusionMatrix.R")
  
  #Get the unique letters set
  letters=uniqueletters(c(ocr_dir,truth_dir))
  
  # Fuzzy Matching & Complete Confusion Matrix
  CANDIDATE_NUM=5
  SIM_THRES=0.5
  Cfs_matrix=confusion(truth_dir,ocr_dir,letters)
  save(Cfs_matrix, letters, file = "../output/Cfs_matrix.RData")
}else  load("../output/Cfs_matrix.RData");

# Visualization
exist=which(Cfs_matrix>0,arr.ind = T)
exist=exist[order(exist[,1]),]

cat("ocr","truth","prob","\n")
for(i in 1:3){
  cat(letters[exist[i,1]],letters[exist[i,2]],Cfs_matrix[exist[i,1],exist[i,2]],"\n")
}
```

## LDA
```{r}
library(topicmodels)
```

# data processing
```{r}
# read in non-error documents and form a corpus
path <- "../data/tesseract/"
data_source <- '../data/ground_truth/'
file_names <- dir(path)
dir_names <- paste(path, file_names, sep = '')
test_ind <- 1:20 * 5
test_set <- file_names[test_ind]
train_set <- file_names[-test_ind]
test_set <- paste(path, test_set, sep = '')
train_set <- paste(data_source, train_set, sep = '')
corpus <- vector('list', length = length(train_set))
for (i in 1:length(train_set)){
  file <- readLines(train_set[i])
  corpus[[i]] <- unlist(strsplit(file[1], split = ' '))
  for (j in 2:length(file)){
    corpus[[i]] <- c(corpus[[i]], unlist(strsplit(file[j], split = ' ')))
  }
  writeLines(file, sub("/data/ground_truth/", "/output/train_set/", train_set[i]))
}
for (i in 1:length(test_set)){
  file <- readLines(test_set[i])
  writeLines(file, sub('delete_error', 'test_set', test_set[i]))
}
save(corpus, file = '../output/corpus.RData')
```
```{r}
# Replace word with word id
vocab <- unique(unlist(corpus))
corpus_wordid <- corpus
for (i in 1:length(corpus)){
  corpus_wordid[[i]] <- match(corpus[[i]], vocab)
}
save(corpus_wordid, file = '../output/corpus_wordid.RData')
```


```{r}
# Generate document-term matrix
doc_term <- matrix(0, length(corpus), length(vocab))
for (i in 1:length(corpus)){
  w_id <- match(corpus[[i]], vocab)
  for (j in 1:length(vocab)){
    doc_term[i,j] <- sum(w_id == j)
  }
}
doc_term[is.na(doc_term)] <- 0
save(doc_term, file = '../output/document_term_matrix.RData')
```

# Train LDA model and generate test set probability
```{r}
lda_topic <- LDA(doc_term, 30)
test_corpus <- vector('list', length = length(test_set))
for (i in 1:length(test_set)){
  file <- readLines(test_set[i])
  test_corpus[[i]] <- file
}
save(test_corpus, file = '../output/test_corpus.RData')
test_doc_term <- matrix(0, length(test_corpus), length(vocab))
for (i in 1:length(test_corpus)){
  test_doc_term[i,] <- match(vocab, test_corpus[[i]])
}
test_doc_term[is.na(test_doc_term)] <- 0
save(test_doc_term, file = '../output/test_doc_term_matrix.RData')
lda_inf <- posterior(lda_topic, test_doc_term)
```

```{r}
# test set document-topic probability
lda_inf$topics
```

```{r}
# Generate test set term-topic probability
lda_inf$terms[,1:10]
```

```{r}
# Output test set topic probabilities
save(lda_inf, file = '../output/test_topic_prob.RData')
```


```{r}
# probabilty of the word
load('../output/test_topic_prob.RData')
prob_of_word = lda_inf$topics %*% lda_inf$terms
prob_of_word = t(prob_of_word)
dim(prob_of_word)
```

```{r}
# total word list
load('../output/corpus.RData')
vocab = unique(unlist(corpus))
yy_prob=prob_of_word
rownames(yy_prob)=vocab
#head(yy_prob,12)
```
### test_corpus
```{r}
load('../output/test_corpus.RData')
```

### candidate_list
```{r}
source("../lib/replace.R")
```


## core_function
```{r}
rownames(Cfs_matrix)=letters
colnames(Cfs_matrix)=letters
bias=1e-5


# New Dict Structure
my_cfs=new.env()
for(i in 1:length(letters)){
  for(j in 1:length(letters)){
    name=paste(letters[i],letters[j],sep = "")
    my_cfs[[name]]=Cfs_matrix[i,j]
  }
}

problda <- function(wc){
  if (exists(wc,envir = my_dict))
    return ( my_dict[[wc]]+bias)
  return (bias)
}

probcf <- function(wc,wi){
  ree = 1
  n=nchar(wc)
  wi=strsplit(wi,"")[[1]]
  wc=strsplit(wc,"")[[1]]
  ind=paste(wi,wc,sep="")
  for(i in ind){
    ree = ree*my_cfs[[i]]
  }
  # ree[is.na(ree)] = 0
  return(ree)
}
```

```{r}
# Score Function
Score <- function(wc,wi){
  return(
    problda(wc) * probcf(wc,wi)
  )
}
```


### Correct Word
```{r}
# generate correct word function
source("../lib/replace.R")
correct <- function(word){
  word_list = Replaces2(word)
  score_list <- sapply(word_list,Score,wi=word)
  return(word_list[which.max(score_list)])
}
```


```{r}
error_path <- '../output/error/'
error_names <- dir('../output/test_set/')
error_dir <- paste(error_path, error_names, sep = '')
error_set <- vector('list', length = length(error_dir))
for (i in 1:length(error_dir)){
  error_set[[i]] <- readLines(error_dir[i], encoding = 'UTF-8')
}
```

### correct words(example)(do not run time consuming!!!)
### We only generated the first document
```{r}
# substitution <- error_set
# for (i in 1:1){
load('../output/substitution.RData')
for (i in 1:length(error_set)){
  my_dict=new.env()
  for(word in vocab){
    my_dict[[word]]=yy_prob[word,i]
  }
  for (j in 1:length(error_set[[i]])){
    substitution[[i]][j] <- correct(error_set[[i]][j])
  }
  writeLines(unlist(substitution[[i]]), sub('/error/', '/substituted/', error_dir[i]))
}
save(substitution, file = '../output/substitution.RData')
```

## Generate full document
```{r}
load('../output/substitution.RData')
load('../output/ifcleanlist.RData')
ifcleanind <- vector('list', 20)
for (i in 1:20){
  ifcleanind[[i]] <- ifcleanlist[[i*5]]
}
new_doc_names <- paste('../data/tesseract/', error_names, sep = '')
new_doc <- vector('list', length = length(error_set))
for (i in 1:length(new_doc)){
  file <- readLines(new_doc_names[i], encoding = 'UTF-8')
  file <- paste(file, collapse = ' ')
  new_doc[[i]] <- unlist(str_split(file, ' ')[[1]])
  sub_ind <- which(ifcleanind[[i]] == FALSE)
  new_doc[[i]][sub_ind] <- substitution[[i]]
  writeLines(unlist(new_doc[[i]]), paste('../output/new_doc/', error_names[i], sep = ''))
}
```


# Step 5 - Performance measure

The two most common OCR accuracy measures are precision and recall. Both are relative measures of the OCR accuracy because they are computed as ratios of the correct output to the total output (precision) or input (recall). More formally defined,
\begin{align*}
\mbox{precision}&=\frac{\mbox{number of correct items}}{\mbox{number of items in OCR output}}\\
\mbox{recall}&=\frac{\mbox{number of correct items}}{\mbox{number of items in ground truth}}
\end{align*}
where *items* refer to either characters or words, and ground truth is the original text stored in the plain text file. 

Both *precision* and *recall* are mathematically convenient measures because their numeric values are some decimal fractions in the range between 0.0 and 1.0, and thus can be written as percentages. For instance, recall is the percentage of words in the original text correctly found by the OCR engine, whereas precision is the percentage of correctly found words with respect to the total word count of the OCR output. Note that in the OCR-related literature, the term OCR accuracy often refers to recall.

Here, we only finished the **word level evaluation** criterions, you are required to complete the **letter-level** part.

##word-level performance measure for error detection
```{r}
OCR_performance_table <- data.frame("Tesseract" = rep(NA,2),
                                    "Tesseract_with_postprocessing" = rep(NA,2))
row.names(OCR_performance_table) <- c("word_wise_recall","word_wise_precision")

r_tw <- NA
r_pw <- NA
p_tw <- NA
p_pw <- NA

for (i in 1:length(file_name_vec)){
current_file_name <- sub(".txt","",file_name_vec[i])

## read ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE,encoding = "UTF-8")
ground_truth_vec <- str_split(paste(current_ground_truth_txt, collapse = " ")," ")[[1]]

## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE,encoding = "UTF-8")
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]]

## read post processed text
tesseract_delete_txt <- readLines(paste("../output/delete_error/",current_file_name,".txt",sep=""),warn = FALSE,encoding = "UTF-8")
tesseract_delete_error_vec <- str_split(paste(tesseract_delete_txt,collapse = " ")," ")[[1]]

#calculate wordwise performance measure
old_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_vec)) 
new_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_delete_error_vec)) 

r_tw[i] <- length(old_intersect_vec)/length(ground_truth_vec)
r_pw[i] <- length(new_intersect_vec)/length(ground_truth_vec)
p_tw[i] <- length(old_intersect_vec)/length(tesseract_vec)
p_pw[i] <- length(new_intersect_vec)/length(tesseract_delete_error_vec)
}

OCR_performance_table["word_wise_recall","Tesseract"] <- mean(r_tw)
OCR_performance_table["word_wise_precision","Tesseract"] <- mean(p_tw)
OCR_performance_table["word_wise_recall","Tesseract_with_postprocessing"] <- mean(r_pw)
OCR_performance_table["word_wise_precision","Tesseract_with_postprocessing"] <- mean(p_pw)

OCR_performance_table
```

#word-level performance measure for error correction
```{r}
OCR_performance_table <- data.frame("Tesseract" = rep(NA,2),
                                    "Tesseract_with_postprocessing" = rep(NA,2))
row.names(OCR_performance_table) <- c("word_wise_recall","word_wise_precision")

r_tw <- NA
r_pw <- NA
p_tw <- NA
p_pw <- NA

correct_path <- "../output/new_doc/"
correct_names <- dir(correct_path)
correct_dir_names <- paste0(correct_path,correct_names,sep='')

truth_path <- "../data/ground_truth/"
post_truth_names <- dir(correct_path)
post_dir_names <- paste0(truth_path, post_truth_names)

tesseract_path <- "../data/tesseract/"
t_dir_names <- paste(tesseract_path,post_truth_names,sep='')

for (i in 1:length(correct_names)){
## read test set ground truth text
 current_ground_truth_txt <- readLines(paste0(post_dir_names[i]), warn=FALSE,encoding = "UTF-8")
    ground_truth_vec <- str_split(paste(current_ground_truth_txt, collapse = " ")," ")[[1]]
ground_truth_vec <- str_split(paste(current_ground_truth_txt, collapse = " ")," ")[[1]]

## read the tesseract text
current_tesseract_txt <- readLines(paste0(t_dir_names[i]), warn=FALSE,encoding = "UTF-8")
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]]

## read post processed text
tesseract_delete_txt <- readLines(paste0(correct_dir_names[i]),warn = FALSE,encoding = "UTF-8")
tesseract_delete_error_vec <- str_split(paste(tesseract_delete_txt,collapse = " ")," ")[[1]]

#calculate wordwise performance measure
old_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_vec)) 
new_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_delete_error_vec)) 

r_tw[i] <- length(old_intersect_vec)/length(ground_truth_vec)
r_pw[i] <- length(new_intersect_vec)/length(ground_truth_vec)
p_tw[i] <- length(old_intersect_vec)/length(tesseract_vec)
p_pw[i] <- length(new_intersect_vec)/length(tesseract_delete_error_vec)
}

OCR_performance_table["word_wise_recall","Tesseract"] <- mean(r_tw)
OCR_performance_table["word_wise_precision","Tesseract"] <- mean(p_tw)
OCR_performance_table["word_wise_recall","Tesseract_with_postprocessing"] <- mean(r_pw)
OCR_performance_table["word_wise_precision","Tesseract_with_postprocessing"] <- mean(p_pw)

OCR_performance_table
```



#letter-level performance measure for error detection
```{r}
noerror_path <- "../output/delete_error/"
file_names <- dir(noerror_path)
noerror_dir_names <- paste(noerror_path, file_names, sep = '')

truth_path <- "../data/ground_truth/"
truth_names <- dir(truth_path)
dir_names <- paste0(truth_path,truth_names)

tesseract_path <- "../data/tesseract/"
t_dir_names <- paste(tesseract_path,truth_names,sep='')

measurement(tesseract_dir=t_dir_names,result_dir = noerror_dir_names,truth_dir = dir_names)

```

#letter-level performance measure for error correction
```{r}
measurement_correct(tesseract_dir=t_dir_names[1],result_dir=correct_dir_names[1],truth_dir=post_dir_names[1])
```

Besides the above required measurement, you are encouraged the explore more evaluation measurements. Here are some related references:

1. Karpinski, R., Lohani, D., & Belaïd, A. *Metrics for Complete Evaluation of OCR Performance*. [pdf](https://csce.ucmss.com/cr/books/2018/LFS/CSREA2018/IPC3481.pdf)

- section 2.1 Text-to-Text evaluation

2. Mei, J., Islam, A., Wu, Y., Moh'd, A., & Milios, E. E. (2016). *Statistical learning for OCR text correction*. arXiv preprint arXiv:1611.06950. [pdf](https://arxiv.org/pdf/1611.06950.pdf)

- section 5, separate the error detection and correction criterions

3. Belaid, A., & Pierron, L. (2001, December). *Generic approach for OCR performance evaluation*. In Document Recognition and Retrieval IX (Vol. 4670, pp. 203-216). International Society for Optics and Photonics. [pdf](https://members.loria.fr/ABelaid/publis/spie02-belaid-pierron.pdf)

- section 3.2, consider the text alignment

# References {-}

1. Borovikov, E. (2014). *A survey of modern optical character recognition techniques*. arXiv preprint arXiv:1412.4183.[pdf](https://pdfs.semanticscholar.org/79c9/cc90b8c2e2c9c54c3862935ea00df7dd56ed.pdf)
(This paper is the source of our evaluation criterion)

2. Kukich, K. (1992). *Techniques for automatically correcting words in text*. Acm Computing Surveys (CSUR), 24(4), 377-439. [pdf](http://www.unige.ch/eti/ptt/docs/kukich-92.pdf)
(This paper is the benchmark review paper)