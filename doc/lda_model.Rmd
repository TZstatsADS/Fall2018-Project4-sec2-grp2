---
title: "LDA model building"
author: "Jiayi Cui, jc4884"
date: "November 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# data processing
```{r}
# read in non-error documents and form a corpus
path <- "../output/delete_error/"
file_names <- dir(path)
dir_names <- paste(path, file_names, sep = '')
test_ind <- rep(1:20) * 5
test_set <- dir_names[test_ind]
train_set <- dir_names[-test_ind]
corpus <- vector('list', length = length(train_set))
for (i in 1:length(train_set)){
  file <- readLines(train_set[i])
  corpus[[i]] <- file
  writeLines(file, sub("delete_error", "train_set", train_set[i]))
}
for (i in 1:length(test_set)){
  file <- readLines(test_set[i])
  writeLines(file, sub('delete_error', 'test_set', test_set[i]))
}
save(corpus, file = '../output/corpus.RData')
```

```{r}
# parameters
topic <- 30
alpha <- 1 # dirichlet prior parameter
eta <- 0.001 # hyperparameter
iter <- 100 # iteration for collapsed of gibbs sampling
```

```{r}
# Replace word with word id
vocab <- unique(unlist(corpus))
corpus_wordid <- corpus
for (i in 1:length(corpus)){
  corpus_wordid[[i]] <- match(corpus[[i]], vocab)
}
save(corpus_wordid, file = '../output/corpus_wordid.RData')
```

```{r}
# Randomly assign topic to each words and generate word-topic count matrix
word_topic <- matrix(0, topic, length(vocab))
# Assign topic 0 to all words
topic_id <- sapply(corpus_wordid, function(x) rep(0, length(x)))
for (i in 1:length(corpus_wordid)){
  for (j in 1:length(corpus_wordid[[i]])){
    topic_id[[i]][j] <- sample(1:topic, 1) # Randomly assign topic
    t_ind <- topic_id[[i]][j] # mark down topic id
    w_ind <- corpus_wordid[[i]][j] # mark down word id
    word_topic[t_ind, w_ind] <- word_topic[t_ind, w_ind] + 1 # word-topic matrix count + 1
  }
}
print(word_topic[1:5, 1:5])
```

The number in row r and column c shows how many times word c is assigned to topic r.

```{r}
# Generate document-topic matrix to sum up numbers of words in each topic of each document
doc_topic <- matrix(0, length(corpus), topic)
for (i in  1:length(corpus)){
  for (t in 1:topic){
    doc_topic[i, t] <- sum(topic_id[[i]] == t)
  }
}
print(doc_topic[1:5, 1:5])
```

# Model training
```{r}
for (i in 1:iter){
  for (j in 1:length(corpus_wordid)){
    for (w in length(corpus_wordid[[j]])){
      t_init <- topic_id[[j]][w] # initial assigned topic
      w_id <- corpus_wordid[[j]][w]
      
      # Use collaspesd Gibbs sampling to calculate probability of a topic is assigned to a word
      doc_topic[j, t_init] <- doc_topic[j, t_init] - 1
      word_topic[t_init, w_id] <- word_topic[t_init, w_id] - 1
      within_doc <- sum(doc_topic[j,]) + topic * alpha # number of words in a document plus number of topic times alpha
      within_vocab <- rowSums(word_topic) + length(vocab) * eta # number of words in each topic plus number of words in vocabulary * eta
      prob <- (word_topic[,w_id] + eta) / within_vocab * (doc_topic[j,] + alpha) / within_doc
      t_new <- sample(1:topic, 1, prob = prob / sum(prob))
      
      topic_id[[j]][w] <- t_new
      doc_topic[j, t_new] <- doc_topic[j, t_new] + 1
      word_topic[t_new, w_id] <- word_topic[t_new, w_id] + 1
    }
  }
}
```

# LDA parameter
```{r}
# Topic probability per document \theta
theta <- (doc_topic + alpha) / rowSums(doc_topic + alpha)
theta[1:10, 1:10]
```

```{r}
# topic probability per word
phi <- (word_topic + eta) / rowSums(word_topic + eta)
phi[1:10, 1:10]
```

```{r}
# Save word-topic probability
save(phi, file = '../output/word_topic_probability.RData')
```

